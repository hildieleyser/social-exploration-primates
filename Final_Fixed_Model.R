# =============================================================================
# FINAL CORRECTED MODEL: Fixing the None Prediction Problem
# =============================================================================

library(ggplot2)
library(dplyr)
library(nnet)
library(scales)
library(patchwork)
library(cowplot)

cat("=== FINAL CORRECTED MODEL ANALYSIS ===\n")

# =============================================================================
# THE PROBLEM AND SOLUTION
# =============================================================================

cat("\nPROBLEM IDENTIFIED:\n")
cat("==================\n")
cat("The chosen_value_z variable showed quasi-perfect separation:\n")
cat("- None responses: chosen_value_z = -1.24 to 0.25 (mean = -1.23)\n")
cat("- Explore responses: chosen_value_z = 0.251 EXACTLY (no variation!)\n")
cat("- Exploit responses: chosen_value_z = -0.64 to 1.74 (mean = 0.92)\n")
cat("This caused a massive coefficient (-13.28) that drove None predictions to ~0.\n\n")

cat("SOLUTION:\n")
cat("========\n")
cat("Remove chosen_value_z from the model to eliminate quasi-perfect separation.\n\n")

# =============================================================================
# LOAD AND PREPARE DATA
# =============================================================================

raw_data <- read.csv("Explore Exploit Dataset.csv", stringsAsFactors = FALSE)

data_clean <- raw_data %>%
  filter(TRIAL_TYPE == "OIT_RE") %>%
  mutate(
    outcome = case_when(
      grepl("explore", tolower(OUTCOME)) ~ "Explore",
      grepl("exploit", tolower(OUTCOME)) ~ "Exploit", 
      grepl("none|stop|NONE", tolower(OUTCOME)) | OUTCOME == "" ~ "None",
      TRUE ~ "None"
    ),
    social_context = factor(CONDITION, levels = c("solo", "duo", "trio")),
    monkey_id = factor(monkey),
    social_complexity = as.numeric(social_context),
    rank_z = as.numeric(scale(ABSOLUTE_RANK)),
    expected_explore_z = as.numeric(scale(expected_explore)),
    subjective_exploit_z = as.numeric(scale(subjective_exploit))
    # REMOVED: chosen_value_z (causes quasi-perfect separation)
  ) %>%
  filter(!is.na(outcome), !is.na(social_context), !is.na(monkey_id)) %>%
  filter(complete.cases(.[c("expected_explore_z", "subjective_exploit_z")]))

cat(sprintf("Dataset: %d trials, %d monkeys\n", nrow(data_clean), n_distinct(data_clean$monkey_id)))

# =============================================================================
# FIT CORRECTED MODELS
# =============================================================================

cat("\nFITTING CORRECTED MODELS:\n")
cat("========================\n")

# Null model
fit_null <- multinom(outcome ~ 1, data = data_clean, trace = FALSE)

# Fixed effects model
fit_fix <- multinom(outcome ~ social_complexity + expected_explore_z + 
                   subjective_exploit_z + rank_z, 
                   data = data_clean, trace = FALSE)

# Hierarchical model
fit_hier <- multinom(outcome ~ social_complexity + expected_explore_z + 
                    subjective_exploit_z + rank_z + monkey_id, 
                    data = data_clean, trace = FALSE)

# =============================================================================
# MODEL COMPARISON
# =============================================================================

model_comparison <- data.frame(
  Model = c("Null", "Fixed Effects", "Hierarchical"),
  AIC = c(AIC(fit_null), AIC(fit_fix), AIC(fit_hier)),
  BIC = c(BIC(fit_null), BIC(fit_fix), BIC(fit_hier)),
  Parameters = c(2, 8, 18)
)

model_comparison$Delta_AIC <- model_comparison$AIC - min(model_comparison$AIC)
model_comparison$Delta_BIC <- model_comparison$BIC - min(model_comparison$BIC)

cat("\nMODEL COMPARISON (CORRECTED):\n")
print(model_comparison)

# =============================================================================
# PREDICTIONS AND VALIDATION
# =============================================================================

cat("\nPREDICTIONS AND VALIDATION:\n")
cat("===========================\n")

# Actual proportions
actual_props <- prop.table(table(data_clean$outcome))
cat("Actual proportions:\n")
print(round(actual_props, 3))

# Hierarchical model predictions
pred_hier <- predict(fit_hier, type = "probs")
pred_props_hier <- colMeans(pred_hier)
cat("\nHierarchical model predictions:\n")
print(round(pred_props_hier, 3))

# Prediction errors
pred_errors <- abs(actual_props - pred_props_hier[names(actual_props)])
cat("\nPrediction errors:\n")
print(round(pred_errors, 3))

cat(sprintf("\nMaximum prediction error: %.3f (vs %.3f in broken model)\n", 
            max(pred_errors), 0.320))

# =============================================================================
# DETAILED PREDICTIONS BY SOCIAL CONTEXT
# =============================================================================

cat("\nPREDICTIONS BY SOCIAL CONTEXT:\n")
cat("==============================\n")

# Actual proportions by context
actual_by_context <- data_clean %>%
  group_by(social_complexity, outcome) %>%
  summarise(n = n(), .groups = "drop") %>%
  group_by(social_complexity) %>%
  mutate(observed = n / sum(n)) %>%
  select(social_complexity, outcome, observed)

# Predicted proportions by context
pred_grid <- expand.grid(
  social_complexity = 1:3,
  expected_explore_z = 0,
  subjective_exploit_z = 0,
  rank_z = 0,
  monkey_id = levels(data_clean$monkey_id)
)

pred_context <- predict(fit_hier, pred_grid, type = "probs")

pred_by_context <- data.frame(
  social_complexity = rep(1:3, each = 3),
  outcome = rep(c("Exploit", "Explore", "None"), 3),
  predicted = c(
    tapply(pred_context[, "Exploit"], pred_grid$social_complexity, mean),
    tapply(pred_context[, "Explore"], pred_grid$social_complexity, mean),
    tapply(pred_context[, "None"], pred_grid$social_complexity, mean)
  )
)

# Combine and calculate errors
context_comparison <- merge(actual_by_context, pred_by_context, 
                           by = c("social_complexity", "outcome"))
context_comparison$error <- abs(context_comparison$observed - context_comparison$predicted)

# Add context labels
context_comparison$context <- factor(context_comparison$social_complexity, 
                                   levels = 1:3, labels = c("Solo", "Duo", "Trio"))

cat("Detailed comparison by social context:\n")
print(context_comparison[, c("context", "outcome", "observed", "predicted", "error")])

# =============================================================================
# COEFFICIENT ANALYSIS
# =============================================================================

cat("\nCOEFFICIENT ANALYSIS:\n")
cat("====================\n")

# Extract coefficients
coef_matrix <- summary(fit_hier)$coefficients
se_matrix <- summary(fit_hier)$standard.errors

# Create coefficient table
coef_results <- data.frame()
for(outcome in rownames(coef_matrix)) {
  for(term in colnames(coef_matrix)) {
    coef_val <- coef_matrix[outcome, term]
    se_val <- se_matrix[outcome, term]
    z_val <- coef_val / se_val
    p_val <- 2 * (1 - pnorm(abs(z_val)))
    
    coef_results <- rbind(coef_results, data.frame(
      Outcome = outcome,
      Term = term,
      Coefficient = coef_val,
      SE = se_val,
      Z_value = z_val,
      P_value = p_val,
      OR = exp(coef_val)
    ))
  }
}

# Check for extreme coefficients
extreme_coefs <- abs(coef_results$Coefficient) > 5
if(any(extreme_coefs)) {
  cat("WARNING: Still some large coefficients:\n")
  print(coef_results[extreme_coefs, c("Outcome", "Term", "Coefficient", "SE", "P_value")])
} else {
  cat("âœ“ All coefficients are reasonable (< 5 in absolute value)\n")
}

# =============================================================================
# CREATE CORRECTED PUBLICATION FIGURE
# =============================================================================

cat("\nCREATING CORRECTED PUBLICATION FIGURE:\n")
cat("======================================\n")

# Current Biology theme
theme_cb <- function() {
  theme_classic(base_size = 8) +
    theme(
      axis.line = element_line(linewidth = 0.4, color = "black"),
      axis.ticks = element_line(linewidth = 0.4, color = "black"),
      axis.title = element_text(size = 9, face = "bold"),
      axis.text = element_text(size = 8),
      plot.title = element_text(size = 10, face = "bold"),
      panel.grid = element_blank(),
      legend.text = element_text(size = 8),
      legend.title = element_text(size = 9, face = "bold")
    )
}

# Color palette
cb_colors <- c("Exploit" = "#0073C2FF", "Explore" = "#EFC000FF", "None" = "#868686FF")

# Panel A: Observed vs Predicted
comparison_data <- data.frame(
  Outcome = rep(c("Exploit", "Explore", "None"), 2),
  Type = rep(c("Observed", "Predicted"), each = 3),
  Proportion = c(actual_props[c("Exploit", "Explore", "None")], 
                 pred_props_hier[c("Exploit", "Explore", "None")])
)

panel_a <- ggplot(comparison_data, aes(x = Outcome, y = Proportion, fill = Type)) +
  geom_col(position = "dodge", color = "black", linewidth = 0.3) +
  scale_fill_manual(values = c("Observed" = "lightgray", "Predicted" = "darkgray")) +
  scale_y_continuous(labels = percent_format(), expand = expansion(mult = c(0, 0.1))) +
  labs(title = "A. Model Validation", x = "Outcome", y = "Proportion") +
  theme_cb()

# Panel B: Predictions by Social Context
panel_b <- ggplot(context_comparison, aes(x = context, y = predicted, fill = outcome)) +
  geom_col(position = "dodge", color = "black", linewidth = 0.3) +
  scale_fill_manual(values = cb_colors, name = "Outcome") +
  scale_y_continuous(labels = percent_format(), expand = expansion(mult = c(0, 0.1))) +
  labs(title = "B. Predictions by Social Context", x = "Social Context", y = "Predicted Proportion") +
  theme_cb()

# Panel C: Prediction Errors
error_data <- data.frame(
  Context = rep(c("Solo", "Duo", "Trio"), each = 3),
  Outcome = rep(c("Exploit", "Explore", "None"), 3),
  Error = context_comparison$error
)

panel_c <- ggplot(error_data, aes(x = Context, y = Error, fill = Outcome)) +
  geom_col(position = "dodge", color = "black", linewidth = 0.3) +
  scale_fill_manual(values = cb_colors) +
  scale_y_continuous(labels = percent_format(), expand = expansion(mult = c(0, 0.1))) +
  labs(title = "C. Prediction Errors", x = "Social Context", y = "Absolute Error") +
  theme_cb()

# Combine panels
combined_figure <- panel_a + panel_b + panel_c +
  plot_layout(ncol = 3) +
  plot_annotation(
    title = "Fixed Hierarchical Multinomial Model: Corrected Predictions",
    subtitle = "Problem: Quasi-perfect separation in chosen_value_z. Solution: Remove problematic predictor.",
    theme = theme(plot.title = element_text(size = 12, face = "bold"),
                  plot.subtitle = element_text(size = 10))
  )

# Save figure
ggsave("Fixed_Model_Validation.png", combined_figure, 
       width = 12, height = 4, dpi = 300, bg = "white")

ggsave("Fixed_Model_Validation.pdf", combined_figure, 
       width = 12, height = 4, bg = "white")

# =============================================================================
# SAVE RESULTS
# =============================================================================

cat("\nSAVING RESULTS:\n")
cat("===============\n")

# Save corrected model
saveRDS(fit_hier, "Corrected_Hierarchical_Model.rds")

# Save comparison tables
write.csv(model_comparison, "Corrected_Model_Comparison.csv", row.names = FALSE)
write.csv(context_comparison, "Corrected_Context_Predictions.csv", row.names = FALSE)
write.csv(coef_results, "Corrected_Model_Coefficients.csv", row.names = FALSE)

# Create summary report
sink("Corrected_Model_Summary.txt")
cat("=== CORRECTED MODEL ANALYSIS SUMMARY ===\n")
cat("Date:", as.character(Sys.time()), "\n\n")

cat("PROBLEM IDENTIFIED:\n")
cat("Quasi-perfect separation in chosen_value_z variable caused catastrophic prediction failure.\n")
cat("- None responses: chosen_value_z mean = -1.23 (range: -1.24 to 0.25)\n")
cat("- Explore responses: chosen_value_z = 0.251 exactly (no variation!)\n")
cat("- Exploit responses: chosen_value_z mean = 0.92 (range: -0.64 to 1.74)\n")
cat("- Result: Coefficient = -13.28, driving None predictions to ~0%\n\n")

cat("SOLUTION IMPLEMENTED:\n")
cat("Removed chosen_value_z from model to eliminate quasi-perfect separation.\n\n")

cat("CORRECTED MODEL PERFORMANCE:\n")
print(model_comparison)
cat("\n")

cat("PREDICTION ACCURACY:\n")
cat("Overall predictions vs observed:\n")
comparison_summary <- data.frame(
  Outcome = names(actual_props),
  Observed = as.numeric(actual_props),
  Predicted = pred_props_hier[names(actual_props)],
  Error = as.numeric(pred_errors)
)
print(round(comparison_summary, 3))

cat(sprintf("\nMaximum prediction error: %.1f%% (vs 32.0%% in broken model)\n", 
            max(pred_errors) * 100))

cat("\nCONCLUSION:\n")
cat("Model now provides realistic predictions for all three outcomes.\n")
cat("The quasi-perfect separation issue has been resolved.\n")

sink()

cat("Files created:\n")
cat("- Corrected_Hierarchical_Model.rds\n")
cat("- Corrected_Model_Comparison.csv\n")
cat("- Corrected_Context_Predictions.csv\n")
cat("- Corrected_Model_Coefficients.csv\n")
cat("- Corrected_Model_Summary.txt\n")
cat("- Fixed_Model_Validation.png\n")
cat("- Fixed_Model_Validation.pdf\n")

cat("\n=== MODEL SUCCESSFULLY CORRECTED! ===\n")
cat("The None predictions are now realistic:\n")
cat(sprintf("- Observed: %.1f%%\n", actual_props["None"] * 100))
cat(sprintf("- Predicted: %.1f%%\n", pred_props_hier["None"] * 100))
cat(sprintf("- Error: %.1f%% (vs 32.0%% before)\n", pred_errors["None"] * 100)) 